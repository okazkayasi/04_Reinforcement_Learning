{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size, memory=20000, \n",
    "                 gamma=0.99, max_eps=1, min_eps=0.1, decay=0.0001,\n",
    "                 lr=0.00025, batch_size=32):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=memory)\n",
    "        \n",
    "        self.max_eps = max_eps\n",
    "        self.epsilon = max_eps\n",
    "        self.min_eps = min_eps\n",
    "        \n",
    "        self.decay = decay\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(self.state_size,),\n",
    "                            activation = 'relu'))\n",
    "        self.model.add(Dense(24, activation='relu'))\n",
    "        self.model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "\n",
    "        self.model.compile(optimizer=Adam(lr=lr), loss='mse')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def act(self, state):\n",
    "        # explore\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # exploit\n",
    "        else:\n",
    "            probs = self.model.predict(state)\n",
    "            return np.argmax(probs[0])\n",
    "        \n",
    "    def add_memory(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    \n",
    "    def batch_run(self):\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        states = []\n",
    "        targets = []\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            state = batch[i][0]\n",
    "            action = batch[i][1]\n",
    "            reward = batch[i][2]\n",
    "            next_state = batch[i][3]\n",
    "            done = batch[i][4]\n",
    "            \n",
    "            probs = self.model.predict(next_state)[0]\n",
    "            target_n = reward + self.gamma * np.amax(probs)\n",
    "            \n",
    "            if done:\n",
    "                target_n = reward\n",
    "                \n",
    "            target = self.model.predict(state)\n",
    "            target[0][action] = target_n\n",
    "            \n",
    "            states.append(state[0])\n",
    "            targets.append(target[0])\n",
    "            \n",
    "        self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n",
    "        \n",
    "        self.steps += 1\n",
    "        self.epsilon = self.min_eps + (self.max_eps - self.min_eps \\\n",
    "                                      ) * math.exp(-self.decay * self.steps)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = Agent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1000, score: -126.44150338966415, e: 0.992739444945014\n",
      "episode: 1/1000, score: -110.61753218465195, e: 0.9819141920283637\n",
      "episode: 2/1000, score: -117.34672464773749, e: 0.9747120647315229\n",
      "episode: 3/1000, score: -169.19005087758796, e: 0.9662683758003969\n",
      "episode: 4/1000, score: -507.2320467177176, e: 0.9567059663831154\n",
      "episode: 5/1000, score: -105.53884188723777, e: 0.9493698854349543\n",
      "episode: 6/1000, score: 27.45003860488046, e: 0.9429391420836651\n",
      "episode: 7/1000, score: -168.20225442528064, e: 0.9328010376890603\n",
      "episode: 8/1000, score: -277.6675572179824, e: 0.926247828748743\n",
      "episode: 9/1000, score: -82.74165704138443, e: 0.9199101518368628\n",
      "episode: 10/1000, score: -90.79308249391228, e: 0.9094012390972809\n",
      "episode: 11/1000, score: -296.4291911228055, e: 0.9017483361708779\n",
      "episode: 12/1000, score: -84.7580894164775, e: 0.8957577165541621\n",
      "episode: 13/1000, score: -121.11261071638313, e: 0.8881549939326455\n",
      "episode: 14/1000, score: -113.48195233256831, e: 0.8803127207115923\n",
      "episode: 15/1000, score: -373.50225460203, e: 0.8737855463695913\n",
      "episode: 16/1000, score: -223.95334609809302, e: 0.8646320695731312\n",
      "episode: 17/1000, score: -211.40226364142092, e: 0.8588428925821529\n",
      "episode: 18/1000, score: -83.60427021517276, e: 0.8530222405783281\n",
      "episode: 19/1000, score: -267.43696933226516, e: 0.8463500778033061\n",
      "episode: 20/1000, score: -94.57558322006209, e: 0.8392193990778053\n",
      "episode: 21/1000, score: -170.0264966371513, e: 0.8325962732583316\n",
      "episode: 22/1000, score: -104.70028385677684, e: 0.820031357629059\n",
      "episode: 23/1000, score: -252.7453067393106, e: 0.8145798612372113\n",
      "episode: 24/1000, score: -218.4577544994619, e: 0.8039411531046766\n",
      "episode: 25/1000, score: -328.5067640473868, e: 0.7962402331311659\n",
      "episode: 26/1000, score: -141.2952220529533, e: 0.7856688349808538\n",
      "episode: 27/1000, score: -342.3789672079573, e: 0.7783034561998208\n",
      "episode: 28/1000, score: -217.19966544871164, e: 0.7688733724236549\n",
      "episode: 29/1000, score: -188.1537911436552, e: 0.760828772596863\n",
      "episode: 30/1000, score: -267.2015969931145, e: 0.7547115541671984\n",
      "episode: 31/1000, score: -114.7927975841139, e: 0.7458677512027456\n",
      "episode: 32/1000, score: -118.5246240658654, e: 0.7410418627716724\n",
      "episode: 33/1000, score: -200.2369654125103, e: 0.7194888876667083\n",
      "episode: 34/1000, score: -358.14867428542584, e: 0.7109986583387822\n",
      "episode: 35/1000, score: -389.903575066314, e: 0.7054637923831322\n",
      "episode: 36/1000, score: -99.02366459869182, e: 0.6988401872828527\n",
      "episode: 37/1000, score: -175.21373449419258, e: 0.6886870765815315\n",
      "episode: 38/1000, score: -286.8250917169296, e: 0.678127691211451\n",
      "episode: 39/1000, score: -348.58675394592785, e: 0.6722035377566261\n",
      "episode: 40/1000, score: -337.6422430170833, e: 0.6639664496679399\n",
      "episode: 41/1000, score: -186.98005409052928, e: 0.6589135238941136\n",
      "episode: 42/1000, score: -295.25168605775957, e: 0.6532415820295535\n",
      "episode: 43/1000, score: -149.30155495619567, e: 0.6452229335035824\n",
      "episode: 44/1000, score: -41.48924080819826, e: 0.6398518571580019\n",
      "episode: 45/1000, score: -95.74110284101074, e: 0.6319208836422546\n",
      "episode: 46/1000, score: -121.14587343414493, e: 0.6249456656964895\n",
      "episode: 47/1000, score: -243.20810871498642, e: 0.617802949400759\n",
      "episode: 48/1000, score: -30.793954450916203, e: 0.6114218374851708\n",
      "episode: 49/1000, score: -278.70480200329416, e: 0.6030023103972656\n",
      "episode: 50/1000, score: -182.68616600776613, e: 0.5948698324917605\n",
      "episode: 51/1000, score: -64.18280377736222, e: 0.585798905375414\n",
      "episode: 52/1000, score: -134.4064692155345, e: 0.5770373374131835\n",
      "episode: 53/1000, score: -268.8214117978488, e: 0.5645154240112853\n",
      "episode: 54/1000, score: -241.3173060588771, e: 0.555135311805984\n",
      "episode: 55/1000, score: -77.66011388709164, e: 0.5483592297950377\n",
      "episode: 56/1000, score: -167.3760786834444, e: 0.5431883120355149\n",
      "episode: 57/1000, score: -135.89340484404136, e: 0.5395250725196344\n",
      "episode: 58/1000, score: -218.8414561816414, e: 0.5338482260277725\n",
      "episode: 59/1000, score: -70.861030636836, e: 0.5218267831592438\n",
      "episode: 60/1000, score: -120.09623219859549, e: 0.5176713015796267\n",
      "episode: 61/1000, score: -62.779788170479016, e: 0.5119470259635446\n",
      "episode: 62/1000, score: -108.30881397642798, e: 0.498734093870827\n",
      "episode: 63/1000, score: -48.66592752606594, e: 0.49413550178147403\n",
      "episode: 64/1000, score: -91.8259715979242, e: 0.48951203496218854\n",
      "episode: 65/1000, score: -42.31325417048299, e: 0.4824870373167315\n",
      "episode: 66/1000, score: -149.7543314496539, e: 0.4761901611659274\n",
      "episode: 67/1000, score: -79.3388752925004, e: 0.47136847482831434\n",
      "episode: 68/1000, score: -251.73722891363045, e: 0.46423336080225375\n",
      "episode: 69/1000, score: -210.4353693426154, e: 0.45605840034977774\n",
      "episode: 70/1000, score: -153.38888660398095, e: 0.4491126897409998\n",
      "episode: 71/1000, score: -71.49623398800938, e: 0.4301987557553453\n",
      "episode: 72/1000, score: -128.9561900798293, e: 0.4238222529743819\n",
      "episode: 73/1000, score: -55.985859898274136, e: 0.4183638029560116\n",
      "episode: 74/1000, score: -121.12281681748368, e: 0.40456811701288475\n",
      "episode: 75/1000, score: -189.6036405869907, e: 0.3893377709687763\n",
      "episode: 76/1000, score: -38.435323092470504, e: 0.3618298232968916\n",
      "episode: 77/1000, score: -211.98072560654282, e: 0.3500084615087071\n",
      "episode: 78/1000, score: -299.71518756809866, e: 0.3433972210622971\n",
      "episode: 79/1000, score: 11.481102265955215, e: 0.33653466289023914\n",
      "episode: 80/1000, score: -238.2986411081219, e: 0.3329665225790031\n",
      "episode: 81/1000, score: -178.0244692584518, e: 0.32689668082636625\n",
      "episode: 82/1000, score: -50.236065767064176, e: 0.3177602666301139\n",
      "episode: 83/1000, score: -32.6071642854307, e: 0.30786623322537426\n",
      "episode: 84/1000, score: -222.43318803272976, e: 0.30322114357633106\n",
      "episode: 85/1000, score: -243.95448469627175, e: 0.2968800703337211\n",
      "episode: 86/1000, score: -75.21527405805429, e: 0.29429833037579367\n",
      "episode: 87/1000, score: -200.87981261847148, e: 0.28810395522354215\n",
      "episode: 88/1000, score: -21.301635448918915, e: 0.28434237519173766\n",
      "episode: 89/1000, score: -215.76241991831796, e: 0.2791627773571977\n",
      "episode: 90/1000, score: -189.19250316984784, e: 0.27275851690181363\n",
      "episode: 91/1000, score: -239.4709669681912, e: 0.261789282512621\n",
      "episode: 92/1000, score: -249.78906529458806, e: 0.2496942324543466\n",
      "episode: 93/1000, score: -390.07937131337263, e: 0.23741351856284598\n",
      "episode: 94/1000, score: -398.3049251544417, e: 0.22683598096756874\n",
      "episode: 95/1000, score: -64.71699092128529, e: 0.21477741870075512\n",
      "episode: 96/1000, score: -126.45568347717565, e: 0.20386528919563396\n",
      "episode: 97/1000, score: -66.0460322274656, e: 0.20029288580846266\n",
      "episode: 98/1000, score: -170.6647415517565, e: 0.19746513880255917\n",
      "episode: 99/1000, score: -99.97342941330002, e: 0.18819892399404353\n",
      "episode: 100/1000, score: -98.55734065947061, e: 0.17981366762802792\n",
      "episode: 101/1000, score: -76.04915218137867, e: 0.1722256151409231\n",
      "episode: 102/1000, score: -86.09669242257688, e: 0.1653589746908569\n",
      "episode: 103/1000, score: -75.39413937380888, e: 0.15914516012504915\n",
      "episode: 104/1000, score: -237.66971931332012, e: 0.15800880232898976\n",
      "episode: 105/1000, score: -69.21759534392056, e: 0.15249378403866512\n",
      "episode: 106/1000, score: -332.3585043546467, e: 0.1515109683840612\n",
      "episode: 107/1000, score: -95.4201119993911, e: 0.14661371277138222\n",
      "episode: 108/1000, score: -92.39268748033352, e: 0.1444735559100731\n",
      "episode: 109/1000, score: -11.709841585636113, e: 0.14329316469564174\n",
      "episode: 110/1000, score: -69.80759730387194, e: 0.13917719288521913\n",
      "episode: 111/1000, score: -98.68066070296636, e: 0.1354525351324149\n",
      "episode: 112/1000, score: -59.57511854508799, e: 0.13208198839047788\n",
      "episode: 113/1000, score: -102.41895342758988, e: 0.1309382958154056\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "batch_size = 32\n",
    "EPISODES = 1000\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    R = 0\n",
    "    for time in range(150000):\n",
    "        # env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward\n",
    "        R += reward\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.add_memory(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {}\".format(e, EPISODES, R, agent.epsilon))\n",
    "            break\n",
    "        agent.batch_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
