{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, theano, keras\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://jaromiru.com/2016/10/03/lets-make-a-dqn-implementation/\n",
    "\n",
    "# Environment\n",
    "#     run()           # runs one episode\n",
    " \n",
    "# Agent\n",
    "#     act(s)          # decides what action to take in state s \n",
    "#     observe(sample) # adds sample (s, a, r, s_) to memory\n",
    "#     replay()        # replays memories and improves\n",
    " \n",
    "# Brain\n",
    "#     predict(s)      # predicts the Q function values in state s\n",
    "#     train(batch)    # performs supervised training step with batch\n",
    " \n",
    "# Memory\n",
    "#     add(sample)     # adds sample to memory\n",
    "#     sample(n)       # returns random batch of n samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating environment class \n",
    "game = 'LunarLander-v2'\n",
    "env = gym.make('LunarLander-v2')\n",
    "sed = 1\n",
    "np.random.seed(sed)\n",
    "random.seed(sed)\n",
    "env.seed(sed)\n",
    "action_size = env.action_space.n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "GAMMA = 0.95\n",
    "\n",
    "INITIAL_EPSILON = 0.5\n",
    "FINAL_EPSILON = 0.001\n",
    "EPSILON_DECAY = 0.99995\n",
    "ALPHA = 0.1\n",
    "TRAIN_BATCH_SIZE = 20\n",
    "REPLAY_MEMORY_SIZE = 20000\n",
    "UPDATE_TARGET_FREQ= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    steps = 0\n",
    "    epsilon = INITIAL_EPSILON\n",
    "    \n",
    "    def __init__(self, stateCnt, actionCnt):\n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "        \n",
    "        self.brain = Brain(stateCnt, actionCnt)\n",
    "        self.memory = Memory(REPLAY_MEMORY_SIZE)\n",
    "        \n",
    "    def act(self, s):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randint(0, self.actionCnt-1)\n",
    "        \n",
    "        else:\n",
    "            return np.argmax(self.brain.predictOne)\n",
    "        \n",
    "    def observe(self, sample):\n",
    "        self.memory.add(sample)\n",
    "        \n",
    "        if self.steps % UPDATE_TARGET_FREQ == 0:\n",
    "            self.brain.updateTargetModel()\n",
    "            \n",
    "        self.steps += 1\n",
    "        self.epsilon = FINAL_EPSILON + (INITIAL_EPSILON - FINAL_EPSILON\n",
    "                        ) * math.exp(-EPSILON_DECAY * self.steps)\n",
    "        \n",
    "    def replay(self):\n",
    "        batch = self.memory.sample(TRAIN_BATCH_SIZE)\n",
    "        batchlen = len(batch)\n",
    "        \n",
    "        no_state = numpy.zeros(self.stateCnt)\n",
    "        states = numpy.array([x[0] for x in batch])\n",
    "        states1 = numpy.array([(-1 if x[3] is None else x[3])\n",
    "                               for o in batch])\n",
    "        \n",
    "        \n",
    "        p = self.brain.predict(states)\n",
    "        p1 = self.brain.predict(states1, target=True)\n",
    "        \n",
    "        x = numpy.zeros((batchlen, self.stateCnt))\n",
    "        y = np.zeros((batchlen, self.actionCnt))\n",
    "        \n",
    "        for i in range(batchlen):\n",
    "            x = batch[i]\n",
    "            s = x[0]\n",
    "            a = x[1]\n",
    "            r = x[2]\n",
    "            s1 = x[3]\n",
    "            \n",
    "            t = p[i]\n",
    "            if s1 is None:\n",
    "                t[a] = r\n",
    "            else:\n",
    "                t[a] = r + GAMMA * np.amax(p1[i])\n",
    "                \n",
    "            xx[i] = s\n",
    "            yy[i] = t\n",
    "            \n",
    "        self.brain.train(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    # The Brain class encapsulates the neural network.\n",
    "    def __init__(self, stateCnt, actionCnt):\n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "        \n",
    "        self.model = Sequential([\n",
    "            Dense(512, input_shape=(stateCnt,)),\n",
    "            Activation('relu'),\n",
    "            Dense(512),\n",
    "            Activation('relu'),\n",
    "            Dense(actionCnt, activation='linear')\n",
    "        ])\n",
    "        opt = Adam(lr=LEARNING_RATE)\n",
    "        self.model.compile(optimizer=opt,\n",
    "                           loss='mse')\n",
    "\n",
    "\n",
    "    def train(self, x, y, epochs=1, verbose=False):\n",
    "        self.model.fit(x,y, batch_size=TRAIN_BATCH_SIZE, epochs=1, verbose=verbose)\n",
    "\n",
    "    def predict(self, s, target=False):\n",
    "        return self.model.predict(s)\n",
    "        \n",
    "    def predictOne(self, s, target=False):\n",
    "        return self.predict(s.reshape(1, self.stateCnt), target=target).flatten()\n",
    "    \n",
    "    def updateTargetModel(self):\n",
    "        self.model.set_weight(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "\n",
    "    def __init__(self, memory):\n",
    "        self.memory = memory\n",
    "        self.replay_memory = []\n",
    "        \n",
    "        \n",
    "    def add(self, sample):\n",
    "        self.replay_memory.append(sample)        \n",
    "\n",
    "        if len(self.replay_memory) > self.memory:\n",
    "            self.replay_memory.pop(0)\n",
    "\n",
    "    def sample(self, n):\n",
    "        n = min(n, len(self.replay_memory))\n",
    "        return random.sample(self.replay_memory, n)\n",
    "\n",
    "    def isFull(self):\n",
    "        return len(self.replay_memory) >= self.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self, problem):\n",
    "        self.problem = problem\n",
    "        self.env = gym.make(problem)\n",
    "        \n",
    "    def run(self, agent):\n",
    "        s =  self.env.reset()\n",
    "        R = 0\n",
    "        \n",
    "        while True:\n",
    "            a = agent.act(s)\n",
    "            \n",
    "            s1, r, done, inf = self.env.step(a)\n",
    "            \n",
    "            if done:\n",
    "                s1 = None\n",
    "                \n",
    "            agent.observe((s,a,r,s1))\n",
    "            agent.replay()\n",
    "            \n",
    "            s = s1\n",
    "            R += r\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        print (\"Total reward: \", R)\n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'set_weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-4e23e592b036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-83-8c6ee81e79c5>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-5c41ba4ba6ae>\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mUPDATE_TARGET_FREQ\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateTargetModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-6bb433131d32>\u001b[0m in \u001b[0;36mupdateTargetModel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdateTargetModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'set_weight'"
     ]
    }
   ],
   "source": [
    "env = Environment('CartPole-v0')\n",
    "stateCnt = env.env.observation_space.shape[0]\n",
    "actionCnt = env.env.action_space.n\n",
    "\n",
    "agent = Agent(stateCnt, actionCnt)\n",
    "\n",
    "while agent.steps < 10000:\n",
    "    env.run(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
