{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size, memory=20000, \n",
    "                 gamma=0.99, max_eps=1, min_eps=0.1, decay=0.001,\n",
    "                 lr=0.00025, batch_size=32):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=memory)\n",
    "        \n",
    "        self.max_eps = max_eps\n",
    "        self.epsilon = max_eps\n",
    "        self.min_eps = min_eps\n",
    "        \n",
    "        self.decay = decay\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = 0\n",
    "                \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(400, input_shape=(self.state_size,),\n",
    "                            activation = 'relu'))\n",
    "        self.model.add(Dense(200, activation='relu'))\n",
    "        self.model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "\n",
    "        self.model.compile(optimizer=Adam(lr=lr), loss='mse')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def act(self, state, test=False):\n",
    "        \n",
    "        if test:\n",
    "            probs = self.model.predict(state)\n",
    "            return np.argmax(probs[0])         \n",
    "        \n",
    "        # explore\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # exploit\n",
    "        else:\n",
    "            probs = self.model.predict(state)\n",
    "            return np.argmax(probs[0])\n",
    "        \n",
    "    def add_memory(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    \n",
    "    def batch_run(self):\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        states = []\n",
    "        targets = []\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            state = batch[i][0]\n",
    "            action = batch[i][1]\n",
    "            reward = batch[i][2]\n",
    "            next_state = batch[i][3]\n",
    "            done = batch[i][4]\n",
    "            \n",
    "            probs = self.model.predict(next_state)[0]\n",
    "            target_n = reward + self.gamma * np.amax(probs)\n",
    "            \n",
    "            if done:\n",
    "                target_n = reward\n",
    "                \n",
    "            target = self.model.predict(state)\n",
    "            target[0][action] = target_n\n",
    "            \n",
    "            states.append(state[0])\n",
    "            targets.append(target[0])\n",
    "            \n",
    "        self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n",
    "        \n",
    "        self.steps += 1\n",
    "        self.epsilon = self.min_eps + (self.max_eps - self.min_eps \\\n",
    "                                      ) * math.exp(-self.decay * self.steps)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = Agent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -297.71387871840636 -100 0.9151684372211937\n",
      "1 -232.730075092128 -100 0.8287966601931487\n",
      "2 -129.2585975719616 -100 0.7774899802355766\n",
      "3 -265.9487179109491 -100 0.7069186506163998\n",
      "4 -93.08859351761338 -100 0.6710031711534054\n",
      "5 -141.08320044889916 -100 0.6302744821437793\n",
      "6 -49.98798955131526 -100 0.5851192627687214\n",
      "7 -318.9893521017554 -100 0.492444357689382\n",
      "8 -76.9389783928394 -100 0.46154798208782744\n",
      "9 -269.0263433790966 -100 0.42845333180904444\n",
      "10 -215.5839504257845 -100 0.3356611017222934\n",
      "11 -170.25987225854158 -100 0.19863190242913717\n",
      "12 -185.8881230215843 -100 0.1639687551471016\n",
      "13 -139.8896768991896 -100 0.15478381279271086\n"
     ]
    }
   ],
   "source": [
    "\n",
    "done = False\n",
    "batch_size = 32\n",
    "TRAIN = 1000\n",
    "r = np.zeros(TRAIN)\n",
    "for e in range(TRAIN):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    R = 0\n",
    "    for time in range(150000):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.add_memory(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(e, R, reward, agent.epsilon)\n",
    "            r[e] = R\n",
    "\n",
    "            break\n",
    "        agent.batch_run()\n",
    "\n",
    "TEST = 100\n",
    "r_test = np.zeros(TEST)\n",
    "for e in range(TEST):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    R = 0\n",
    "    for time in range(150000):\n",
    "        tet = True\n",
    "        # env.render()\n",
    "        action = agent.act(state, test=True)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.add_memory(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(e, R, reward, agent.epsilon)\n",
    "            r_test[e] = R\n",
    "\n",
    "            break\n",
    "        agent.batch_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269.61857044378775"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(r_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
