{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size, memory=20000, \n",
    "                 gamma=0.99, max_eps=1, min_eps=0.1, decay=0.001,\n",
    "                 lr=0.00025, batch_size=32):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=memory)\n",
    "        \n",
    "        self.max_eps = max_eps\n",
    "        self.epsilon = max_eps\n",
    "        self.min_eps = min_eps\n",
    "        \n",
    "        self.decay = decay\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = 0\n",
    "                \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(400, input_shape=(self.state_size,),\n",
    "                            activation = 'relu'))\n",
    "        self.model.add(Dense(200, activation='relu'))\n",
    "        self.model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "\n",
    "        self.model.compile(optimizer=Adam(lr=lr), loss='mse')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def act(self, state, test=False):\n",
    "        \n",
    "        if test:\n",
    "            probs = self.model.predict(state)\n",
    "            return np.argmax(probs[0])         \n",
    "        \n",
    "        # explore\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # exploit\n",
    "        else:\n",
    "            probs = self.model.predict(state)\n",
    "            return np.argmax(probs[0])\n",
    "        \n",
    "    def add_memory(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    \n",
    "    def batch_run(self):\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        states = []\n",
    "        targets = []\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            state = batch[i][0]\n",
    "            action = batch[i][1]\n",
    "            reward = batch[i][2]\n",
    "            next_state = batch[i][3]\n",
    "            done = batch[i][4]\n",
    "            \n",
    "            probs = self.model.predict(next_state)[0]\n",
    "            target_n = reward + self.gamma * np.amax(probs)\n",
    "            \n",
    "            if done:\n",
    "                target_n = reward\n",
    "                \n",
    "            target = self.model.predict(state)\n",
    "            target[0][action] = target_n\n",
    "            \n",
    "            states.append(state[0])\n",
    "            targets.append(target[0])\n",
    "            \n",
    "        self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n",
    "        \n",
    "        self.steps += 1\n",
    "        self.epsilon = self.min_eps + (self.max_eps - self.min_eps \\\n",
    "                                      ) * math.exp(-self.decay * self.steps)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = Agent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -311.1311495062664 -100 0.9184356410214083\n",
      "1 -106.72216322838148 -100 0.8547561850033666\n",
      "2 -112.0670113330076 -100 0.7842988677347938\n",
      "3 -192.13321330422554 -100 0.7310560988877152\n",
      "4 -239.37052364494622 -100 0.6784746719099781\n",
      "5 -145.3440035544929 -100 0.5890157821670499\n",
      "6 -195.40436680559012 -100 0.5268338226352179\n",
      "7 -420.7253261585752 -100 0.4800849435220095\n",
      "8 -331.69698710302373 -100 0.43643156765091984\n",
      "9 -94.62703420488683 -100 0.37517156154884057\n",
      "10 -93.16218453485034 -100 0.3130218544130568\n",
      "11 -73.4316016981215 -1.7982364810483518 0.17844476631578318\n",
      "12 -76.33710921263473 -1.749647618298613 0.12888708944578908\n",
      "13 -57.438678065121465 -2.8582463038041355 0.11063759860396351\n",
      "14 -99.593279264369 1.4823482817948854 0.10391726914099135\n",
      "15 -267.01816656139897 -100 0.1014939068917623\n",
      "16 -62.64482104476486 -1.72630796077421 0.10055012748501663\n",
      "17 -46.19211064334441 2.8913139189662345 0.10020258307357677\n",
      "18 -24.364471473314165 -1.0582940615690888 0.10007460071132161\n",
      "19 -18.465694608428198 0.14210890544995294 0.10002747152578659\n",
      "20 -43.75073876483335 0.22380610858595845 0.10001011632081884\n",
      "21 -21.552463868539547 0.4571249296300664 0.10000372530989741\n"
     ]
    }
   ],
   "source": [
    "\n",
    "done = False\n",
    "batch_size = 32\n",
    "TRAIN = 1000\n",
    "r = np.zeros(TRAIN)\n",
    "for e in range(TRAIN):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    R = 0\n",
    "    for time in range(150000):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.add_memory(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(e, R, reward, agent.epsilon)\n",
    "            r[e] = R\n",
    "\n",
    "            break\n",
    "        agent.batch_run()\n",
    "\n",
    "TEST = 100\n",
    "r_test = np.zeros(TEST)\n",
    "for e in range(TEST):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    R = 0\n",
    "    for time in range(150000):\n",
    "        tet = True\n",
    "        # env.render()\n",
    "        action = agent.act(state, test=True)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.add_memory(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(e, R, reward, agent.epsilon)\n",
    "            r_test[e] = R\n",
    "\n",
    "            break\n",
    "        agent.batch_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269.61857044378775"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(r_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
