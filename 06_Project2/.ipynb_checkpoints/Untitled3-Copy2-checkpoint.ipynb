{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size, memory=20000, \n",
    "                 gamma=0.99, max_eps=1, min_eps=0.1, decay=0.001,\n",
    "                 lr=0.00025, batch_size=32):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=memory)\n",
    "        \n",
    "        self.max_eps = max_eps\n",
    "        self.epsilon = max_eps\n",
    "        self.min_eps = min_eps\n",
    "        \n",
    "        self.decay = decay\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = 0\n",
    "                \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(self.state_size,),\n",
    "                            activation = 'relu'))\n",
    "        self.model.add(Dense(24, activation='relu'))\n",
    "        self.model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "\n",
    "        self.model.compile(optimizer=Adam(lr=lr), loss='mse')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def act(self, state, test=False):\n",
    "        \n",
    "        if test:\n",
    "            probs = self.model.predict(state)\n",
    "            return np.argmax(probs[0])         \n",
    "        \n",
    "        # explore\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # exploit\n",
    "        else:\n",
    "            probs = self.model.predict(state)\n",
    "            return np.argmax(probs[0])\n",
    "        \n",
    "    def add_memory(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    \n",
    "    def batch_run(self):\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        states = []\n",
    "        targets = []\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            state = batch[i][0]\n",
    "            action = batch[i][1]\n",
    "            reward = batch[i][2]\n",
    "            next_state = batch[i][3]\n",
    "            done = batch[i][4]\n",
    "            \n",
    "            probs = self.model.predict(next_state)[0]\n",
    "            target_n = reward + self.gamma * np.amax(probs)\n",
    "            \n",
    "            if done:\n",
    "                target_n = reward\n",
    "                \n",
    "            target = self.model.predict(state)\n",
    "            target[0][action] = target_n\n",
    "            \n",
    "            states.append(state[0])\n",
    "            targets.append(target[0])\n",
    "            \n",
    "        self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n",
    "        \n",
    "        self.steps += 1\n",
    "        self.epsilon = self.min_eps + (self.max_eps - self.min_eps \\\n",
    "                                      ) * math.exp(-self.decay * self.steps)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = Agent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running(batch_size, gamma, lr):\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = Agent(state_size, action_size, batch_size=batch_size, gamma=gamma, lr=lr)\n",
    "    done = False\n",
    "    EPISODES = 1000\n",
    "    for e in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        R = 0\n",
    "        for time in range(150000):\n",
    "            # env.render()\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = reward\n",
    "            R += reward\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.add_memory(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}, e: {}\".format(e, EPISODES, R, agent.epsilon))\n",
    "                break\n",
    "            agent.batch_run()\n",
    "    RR = np.zeros(200)\n",
    "    for e in range(200):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        R = 0\n",
    "        for time in range(150000):\n",
    "            # env.render()\n",
    "            action = agent.act(state, test)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = reward\n",
    "            R += reward\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.add_memory(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}, e: {}\".format(e, EPISODES, R, agent.epsilon))\n",
    "                RR[e] = R\n",
    "                break\n",
    "            agent.batch_run()\n",
    "            \n",
    "    return RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "batches = [25, 50]\n",
    "gammas = [0.99, 0.95]\n",
    "lrs = [0.00025, 0.0005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1000, score: -173.48172027352234, e: 0.9159840133785289\n",
      "episode: 1/1000, score: -302.36243869181544, e: 0.8487422234440046\n",
      "episode: 2/1000, score: -111.27589489543733, e: 0.7884170029925776\n",
      "episode: 3/1000, score: -326.84168368056106, e: 0.7297952479611499\n",
      "episode: 4/1000, score: -270.9478873104229, e: 0.6732917579601843\n",
      "episode: 5/1000, score: -215.12366563571197, e: 0.6187358340539895\n",
      "episode: 6/1000, score: -325.7981968921416, e: 0.5755132577614519\n",
      "episode: 7/1000, score: -438.2661834921166, e: 0.5363282221906089\n",
      "episode: 8/1000, score: -444.6182507229579, e: 0.49638848905539934\n",
      "episode: 9/1000, score: -235.84081247038856, e: 0.46154798208782744\n",
      "episode: 10/1000, score: -326.3457763647187, e: 0.431754342673077\n",
      "episode: 11/1000, score: -459.5348068611722, e: 0.3980897940177829\n",
      "episode: 12/1000, score: -264.8182128446083, e: 0.36972279956192167\n",
      "episode: 13/1000, score: -362.05448361937147, e: 0.35224346784143046\n",
      "episode: 14/1000, score: -354.1704940544349, e: 0.33755395184135817\n",
      "episode: 15/1000, score: -334.9244648517961, e: 0.31430382761321835\n",
      "episode: 16/1000, score: -550.188866405916, e: 0.29429833037579367\n",
      "episode: 17/1000, score: -89.5831417173078, e: 0.17154987531148708\n",
      "episode: 18/1000, score: -760.8916060220043, e: 0.1498687604350299\n",
      "episode: 19/1000, score: -135.67180800121068, e: 0.14539467299832207\n",
      "episode: 20/1000, score: 45.174149770670084, e: 0.14026146320030436\n",
      "episode: 21/1000, score: -65.57235415935367, e: 0.13553061656779228\n",
      "episode: 22/1000, score: 123.06099528474323, e: 0.1190753087533823\n",
      "episode: 23/1000, score: -189.32748008365212, e: 0.11367261275874174\n",
      "episode: 24/1000, score: -149.39718101605794, e: 0.11155819562576072\n",
      "episode: 25/1000, score: -111.4032581056376, e: 0.10892088148880516\n",
      "episode: 26/1000, score: -190.00489034205737, e: 0.10586143681216643\n",
      "episode: 27/1000, score: -320.9692540809778, e: 0.10220867936237253\n",
      "episode: 28/1000, score: -431.60507414688396, e: 0.10125031405445811\n",
      "episode: 29/1000, score: -185.59216723004218, e: 0.10046042503053759\n",
      "episode: 30/1000, score: -258.6135808805092, e: 0.10016955036855714\n",
      "episode: 31/1000, score: -96.98942538981017, e: 0.10015037768692625\n",
      "episode: 32/1000, score: -184.56205819272589, e: 0.10005537620796018\n",
      "episode: 33/1000, score: -156.3561523300075, e: 0.1000203921503963\n",
      "episode: 34/1000, score: -130.00555577618638, e: 0.10000750935849714\n",
      "episode: 35/1000, score: -135.3205428280583, e: 0.10000276530252783\n",
      "episode: 36/1000, score: -146.14571811775116, e: 0.10000101831575538\n",
      "episode: 37/1000, score: -102.04795317094873, e: 0.10000086861963162\n",
      "episode: 38/1000, score: -124.47377739596531, e: 0.1000003198670118\n",
      "episode: 39/1000, score: -115.74595664804583, e: 0.10000011779022891\n",
      "episode: 40/1000, score: -92.75363740654882, e: 0.10000004337595787\n",
      "episode: 41/1000, score: -78.64896545442063, e: 0.10000001597308825\n",
      "episode: 42/1000, score: -7.793589160561733, e: 0.1000000058820499\n",
      "episode: 43/1000, score: -83.87731754799823, e: 0.10000000216605019\n",
      "episode: 44/1000, score: -79.40141683258732, e: 0.10000000079764258\n",
      "episode: 45/1000, score: -122.14934708157385, e: 0.1000000002937299\n",
      "episode: 46/1000, score: -134.01529336669634, e: 0.1000000001081653\n",
      "episode: 47/1000, score: -128.7818722546339, e: 0.1000000000398316\n",
      "episode: 48/1000, score: -89.71282585167546, e: 0.1000000000146679\n",
      "episode: 49/1000, score: -111.017731320228, e: 0.10000000000540142\n",
      "episode: 50/1000, score: -113.08589064507284, e: 0.10000000000198907\n",
      "episode: 51/1000, score: -111.55847723136372, e: 0.10000000000073248\n",
      "episode: 52/1000, score: -112.40274865446581, e: 0.10000000000026973\n",
      "episode: 53/1000, score: -138.24469072328796, e: 0.10000000000009933\n",
      "episode: 54/1000, score: -286.9958481910244, e: 0.10000000000008819\n",
      "episode: 55/1000, score: -391.5389267439199, e: 0.10000000000007307\n",
      "episode: 56/1000, score: -304.66610899012517, e: 0.10000000000005965\n",
      "episode: 57/1000, score: -78.22286443339645, e: 0.10000000000002197\n",
      "episode: 58/1000, score: -99.61820140741561, e: 0.1000000000000081\n",
      "episode: 59/1000, score: -315.27540884929584, e: 0.1000000000000066\n",
      "episode: 60/1000, score: -80.94697693517637, e: 0.10000000000000243\n",
      "episode: 61/1000, score: -465.7295396276397, e: 0.10000000000000198\n",
      "episode: 62/1000, score: -251.8382035318408, e: 0.1000000000000016\n",
      "episode: 63/1000, score: -315.69161042297685, e: 0.10000000000000132\n",
      "episode: 64/1000, score: -103.72123336660034, e: 0.10000000000000049\n",
      "episode: 65/1000, score: -114.85747616760239, e: 0.10000000000000019\n",
      "episode: 66/1000, score: -136.03525061471555, e: 0.10000000000000007\n",
      "episode: 67/1000, score: -99.1805176690171, e: 0.10000000000000003\n",
      "episode: 68/1000, score: -136.0817917720585, e: 0.10000000000000002\n",
      "episode: 69/1000, score: -121.22300195041895, e: 0.1\n",
      "episode: 70/1000, score: -122.32222117126788, e: 0.1\n",
      "episode: 71/1000, score: -109.33911466810312, e: 0.1\n",
      "episode: 72/1000, score: -133.10877970814448, e: 0.1\n",
      "episode: 73/1000, score: -126.6704576501667, e: 0.1\n",
      "episode: 74/1000, score: -66.91145617127822, e: 0.1\n",
      "episode: 75/1000, score: -145.19330296724956, e: 0.1\n",
      "episode: 76/1000, score: -94.20823996425845, e: 0.1\n",
      "episode: 77/1000, score: -108.35010317857665, e: 0.1\n",
      "episode: 78/1000, score: -150.39232882766728, e: 0.1\n",
      "episode: 79/1000, score: -86.6804420445207, e: 0.1\n",
      "episode: 80/1000, score: -113.85190895341533, e: 0.1\n",
      "episode: 81/1000, score: -142.9593872244071, e: 0.1\n",
      "episode: 82/1000, score: -94.30242908462556, e: 0.1\n",
      "episode: 83/1000, score: -69.11780779160611, e: 0.1\n",
      "episode: 84/1000, score: -97.12262523761272, e: 0.1\n",
      "episode: 85/1000, score: -114.27861715903485, e: 0.1\n",
      "episode: 86/1000, score: -113.21446164536667, e: 0.1\n",
      "episode: 87/1000, score: -76.1548828262606, e: 0.1\n",
      "episode: 88/1000, score: -104.71593776602516, e: 0.1\n",
      "episode: 89/1000, score: -90.33393713875994, e: 0.1\n",
      "episode: 90/1000, score: -328.230573619259, e: 0.1\n",
      "episode: 91/1000, score: -108.64848814724999, e: 0.1\n",
      "episode: 92/1000, score: -46.90617415575803, e: 0.1\n",
      "episode: 93/1000, score: -104.6456589940025, e: 0.1\n",
      "episode: 94/1000, score: -87.13317328787787, e: 0.1\n",
      "episode: 95/1000, score: -101.81582914875268, e: 0.1\n",
      "episode: 96/1000, score: -294.7997172335974, e: 0.1\n",
      "episode: 97/1000, score: -201.65702833711129, e: 0.1\n",
      "episode: 98/1000, score: 191.28199243426803, e: 0.1\n",
      "episode: 99/1000, score: -44.551191297984595, e: 0.1\n",
      "episode: 100/1000, score: -117.8476755639255, e: 0.1\n",
      "episode: 101/1000, score: -92.37112350544903, e: 0.1\n",
      "episode: 102/1000, score: -81.90137263357853, e: 0.1\n",
      "episode: 103/1000, score: -26.371131155259505, e: 0.1\n",
      "episode: 104/1000, score: 79.30793869232119, e: 0.1\n",
      "episode: 105/1000, score: -57.29319283779293, e: 0.1\n",
      "episode: 106/1000, score: -72.51528330689388, e: 0.1\n",
      "episode: 107/1000, score: -331.3068954634264, e: 0.1\n",
      "episode: 108/1000, score: -150.19153429495188, e: 0.1\n",
      "episode: 109/1000, score: -90.1678273661728, e: 0.1\n",
      "episode: 110/1000, score: -45.28239631999131, e: 0.1\n",
      "episode: 111/1000, score: -69.78644371727333, e: 0.1\n",
      "episode: 112/1000, score: -61.19419431214902, e: 0.1\n",
      "episode: 113/1000, score: -88.28129949602935, e: 0.1\n",
      "episode: 114/1000, score: -94.33338993202389, e: 0.1\n",
      "episode: 115/1000, score: -87.93877150468691, e: 0.1\n",
      "episode: 116/1000, score: -209.67787219026732, e: 0.1\n",
      "episode: 117/1000, score: 37.319220476067414, e: 0.1\n",
      "episode: 118/1000, score: -88.22217835662103, e: 0.1\n",
      "episode: 119/1000, score: -64.165822649544, e: 0.1\n",
      "episode: 120/1000, score: -95.79012559184919, e: 0.1\n",
      "episode: 121/1000, score: -229.1996692920611, e: 0.1\n",
      "episode: 122/1000, score: 9.23785869945661, e: 0.1\n",
      "episode: 123/1000, score: -127.14522928418933, e: 0.1\n",
      "episode: 124/1000, score: -126.44196371290674, e: 0.1\n",
      "episode: 125/1000, score: -146.50898514591503, e: 0.1\n",
      "episode: 126/1000, score: -75.72205126280075, e: 0.1\n",
      "episode: 127/1000, score: -59.85369260637783, e: 0.1\n",
      "episode: 128/1000, score: -243.30650061602466, e: 0.1\n",
      "episode: 129/1000, score: -86.70817653833278, e: 0.1\n",
      "episode: 130/1000, score: 164.04636844128518, e: 0.1\n",
      "episode: 131/1000, score: 36.759344446570495, e: 0.1\n",
      "episode: 132/1000, score: -64.84149791547148, e: 0.1\n",
      "episode: 133/1000, score: -69.98649239743091, e: 0.1\n",
      "episode: 134/1000, score: -251.7483798474236, e: 0.1\n",
      "episode: 135/1000, score: -26.15758231423598, e: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 136/1000, score: 127.37701095296589, e: 0.1\n",
      "episode: 137/1000, score: -27.257986684137563, e: 0.1\n",
      "episode: 138/1000, score: -6.69720590314968, e: 0.1\n",
      "episode: 139/1000, score: -54.0421438612441, e: 0.1\n",
      "episode: 140/1000, score: -32.09040987841634, e: 0.1\n",
      "episode: 141/1000, score: -89.43874355915655, e: 0.1\n",
      "episode: 142/1000, score: 6.953151584742307, e: 0.1\n",
      "episode: 143/1000, score: 17.08191603169702, e: 0.1\n",
      "episode: 144/1000, score: -146.59617863126067, e: 0.1\n",
      "episode: 145/1000, score: -172.45355065980002, e: 0.1\n",
      "episode: 146/1000, score: -58.80704741503926, e: 0.1\n",
      "episode: 147/1000, score: -121.20322206901022, e: 0.1\n",
      "episode: 148/1000, score: 77.76957143672433, e: 0.1\n",
      "episode: 149/1000, score: -113.69434258448966, e: 0.1\n",
      "episode: 150/1000, score: -71.54978625366923, e: 0.1\n",
      "episode: 151/1000, score: -54.669436671613006, e: 0.1\n",
      "episode: 152/1000, score: 81.05557122521752, e: 0.1\n",
      "episode: 153/1000, score: -220.5208589952004, e: 0.1\n",
      "episode: 154/1000, score: -134.8536903910868, e: 0.1\n",
      "episode: 155/1000, score: 180.55748481038225, e: 0.1\n",
      "episode: 156/1000, score: 64.1455165246156, e: 0.1\n",
      "episode: 157/1000, score: -38.59716349689536, e: 0.1\n",
      "episode: 158/1000, score: -106.43768532594129, e: 0.1\n",
      "episode: 159/1000, score: -68.54874615989685, e: 0.1\n",
      "episode: 160/1000, score: -28.374836193414055, e: 0.1\n",
      "episode: 161/1000, score: -154.6987700586972, e: 0.1\n",
      "episode: 162/1000, score: -188.1298495063417, e: 0.1\n",
      "episode: 163/1000, score: -22.42607765695257, e: 0.1\n",
      "episode: 164/1000, score: 200.69121467681867, e: 0.1\n",
      "episode: 165/1000, score: -67.07891547653952, e: 0.1\n",
      "episode: 166/1000, score: 31.25030334470056, e: 0.1\n",
      "episode: 167/1000, score: 30.918652883237154, e: 0.1\n",
      "episode: 168/1000, score: -82.11540995981095, e: 0.1\n",
      "episode: 169/1000, score: 92.81282124133622, e: 0.1\n"
     ]
    }
   ],
   "source": [
    "for b in batches:\n",
    "    for g in gammas:\n",
    "        for l in lrs:\n",
    "            a.append(running(batch_size=b, gamma=g, lr=l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
