{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size, memory=20000, \n",
    "                 gamma=0.99, max_eps=1, min_eps=0.1, decay=0.001,\n",
    "                 lr=0.00025, batch_size=32):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=memory)\n",
    "        \n",
    "        self.max_eps = max_eps\n",
    "        self.epsilon = max_eps\n",
    "        self.min_eps = min_eps\n",
    "        \n",
    "        self.decay = decay\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = 0\n",
    "                \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(self.state_size,),\n",
    "                            activation = 'relu'))\n",
    "        self.model.add(Dense(24, activation='relu'))\n",
    "        self.model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "\n",
    "        self.model.compile(optimizer=Adam(lr=lr), loss='mse')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def act(self, state, test=False):\n",
    "        \n",
    "        if test:\n",
    "            probs = self.model.predict(state)\n",
    "            return np.argmax(probs[0])         \n",
    "        \n",
    "        # explore\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # exploit\n",
    "        else:\n",
    "            probs = self.model.predict(state)\n",
    "            return np.argmax(probs[0])\n",
    "        \n",
    "    def add_memory(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    \n",
    "    def batch_run(self):\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        states = []\n",
    "        targets = []\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            state = batch[i][0]\n",
    "            action = batch[i][1]\n",
    "            reward = batch[i][2]\n",
    "            next_state = batch[i][3]\n",
    "            done = batch[i][4]\n",
    "            \n",
    "            probs = self.model.predict(next_state)[0]\n",
    "            target_n = reward + self.gamma * np.amax(probs)\n",
    "            \n",
    "            if done:\n",
    "                target_n = reward\n",
    "                \n",
    "            target = self.model.predict(state)\n",
    "            target[0][action] = target_n\n",
    "            \n",
    "            states.append(state[0])\n",
    "            targets.append(target[0])\n",
    "            \n",
    "        self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n",
    "        \n",
    "        self.steps += 1\n",
    "        self.epsilon = self.min_eps + (self.max_eps - self.min_eps \\\n",
    "                                      ) * math.exp(-self.decay * self.steps)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = Agent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1000, score: -135.19624625509954, e: 0.9308047117479722\n",
      "episode: 1/1000, score: -265.39557394052474, e: 0.874637178782552\n",
      "episode: 2/1000, score: -266.656369274285, e: 0.7974248481649728\n",
      "episode: 3/1000, score: -440.26747127970816, e: 0.6591371340227155\n",
      "episode: 4/1000, score: -273.74526370339424, e: 0.6244734271365907\n",
      "episode: 5/1000, score: -314.45317671673934, e: 0.5856046246719949\n",
      "episode: 6/1000, score: -387.9531058354748, e: 0.5289733360746749\n",
      "episode: 7/1000, score: -195.3737681243113, e: 0.4900967414348538\n",
      "episode: 8/1000, score: -45.838132707307864, e: 0.46154798208782744\n",
      "episode: 9/1000, score: -75.87697694523476, e: 0.42583618768913867\n",
      "episode: 10/1000, score: -58.85719218492597, e: 0.3948287747713206\n",
      "episode: 11/1000, score: -64.73013909516713, e: 0.3702627849664549\n",
      "episode: 12/1000, score: -10.161406385935337, e: 0.3514878713992665\n",
      "episode: 13/1000, score: -157.59068031287478, e: 0.323943718440868\n",
      "episode: 14/1000, score: -162.2570132472859, e: 0.2716392281711527\n",
      "episode: 15/1000, score: -475.1837973853339, e: 0.2582846134238588\n",
      "episode: 16/1000, score: -129.44243649872908, e: 0.23102586720966234\n",
      "episode: 17/1000, score: -232.06696418518112, e: 0.20317171781667573\n",
      "episode: 18/1000, score: -220.59453215505155, e: 0.1860040822649749\n",
      "episode: 19/1000, score: -258.2308554243274, e: 0.17813161397274246\n",
      "episode: 20/1000, score: -113.53201333307454, e: 0.17062574678264866\n",
      "episode: 21/1000, score: -61.842429986952524, e: 0.16226470692874911\n",
      "episode: 22/1000, score: -264.90648190693815, e: 0.14996859775994575\n",
      "episode: 23/1000, score: -407.5563896616821, e: 0.14018102074317504\n",
      "episode: 24/1000, score: -309.24516277766895, e: 0.1372031994136579\n",
      "episode: 25/1000, score: -257.9019561196067, e: 0.12877177187690042\n",
      "episode: 26/1000, score: -295.94857899551425, e: 0.12315926145372395\n",
      "episode: 27/1000, score: -271.1895618151809, e: 0.11810874124352667\n",
      "episode: 28/1000, score: -205.96136454965773, e: 0.11264668041974144\n",
      "episode: 29/1000, score: -440.23434356020863, e: 0.10951048394546739\n",
      "episode: 30/1000, score: -41.710303422149195, e: 0.10821855796248953\n",
      "episode: 31/1000, score: -150.2600309560168, e: 0.1067019247638319\n",
      "episode: 32/1000, score: -163.60795583865178, e: 0.10559232299426027\n",
      "episode: 33/1000, score: -190.28147186663938, e: 0.10514690088339293\n",
      "episode: 34/1000, score: -205.6002213037346, e: 0.1046061610374454\n",
      "episode: 35/1000, score: -191.09448012795735, e: 0.10392118836941995\n",
      "episode: 36/1000, score: -337.07403221105477, e: 0.10314682737681136\n",
      "episode: 37/1000, score: -260.9740403024274, e: 0.10252538812383406\n",
      "episode: 38/1000, score: -182.00053031066162, e: 0.10177783221135911\n",
      "episode: 39/1000, score: -667.1798386526547, e: 0.10128324851254221\n",
      "episode: 40/1000, score: -385.8118823860082, e: 0.10101856498913578\n",
      "episode: 41/1000, score: -306.7510457405589, e: 0.1007956425620203\n",
      "episode: 42/1000, score: -152.5036197487849, e: 0.10068893714628781\n",
      "episode: 43/1000, score: -275.57205456814006, e: 0.10061286828435789\n",
      "episode: 44/1000, score: -300.5428985241537, e: 0.10041911696860255\n",
      "episode: 45/1000, score: -247.94636679479052, e: 0.10036582315819574\n",
      "episode: 46/1000, score: -365.6546184860206, e: 0.10026273608317093\n",
      "episode: 47/1000, score: -359.2084468740271, e: 0.10018794512695126\n",
      "episode: 48/1000, score: -141.82258922591228, e: 0.1001607981806486\n",
      "episode: 49/1000, score: 20.41869763895151, e: 0.10005921352877027\n",
      "episode: 50/1000, score: -168.21525606090864, e: 0.10005152927239981\n",
      "episode: 51/1000, score: -118.65039319217372, e: 0.10001897552597579\n",
      "episode: 52/1000, score: -235.8447985694531, e: 0.10001662903131578\n",
      "episode: 53/1000, score: -272.53619799194644, e: 0.10001483738549012\n",
      "episode: 54/1000, score: -174.94581703277336, e: 0.10000546383018173\n",
      "episode: 55/1000, score: -108.09958702574356, e: 0.10000201204183005\n",
      "episode: 56/1000, score: -228.97012400722488, e: 0.10000133930395483\n",
      "episode: 57/1000, score: -217.16000700637971, e: 0.10000110865774217\n",
      "episode: 58/1000, score: -70.97785295667981, e: 0.10000040826044702\n",
      "episode: 59/1000, score: -163.83815522737797, e: 0.10000023531035945\n",
      "episode: 60/1000, score: -226.83643510985877, e: 0.10000010394951217\n",
      "episode: 61/1000, score: -155.23910551774486, e: 0.10000003827914847\n",
      "episode: 62/1000, score: -97.2745213176918, e: 0.1000000140962009\n",
      "episode: 63/1000, score: -152.14075842043428, e: 0.1000000051908908\n",
      "episode: 64/1000, score: -294.0083549411487, e: 0.10000000448126478\n",
      "episode: 65/1000, score: -77.70184227597693, e: 0.10000000165021457\n",
      "episode: 66/1000, score: -66.64536263863921, e: 0.1000000006076874\n",
      "episode: 67/1000, score: -24.06961505510817, e: 0.10000000022377938\n",
      "episode: 68/1000, score: -134.7162066247777, e: 0.1000000000824062\n",
      "episode: 69/1000, score: -226.1541191247152, e: 0.10000000003589725\n",
      "episode: 70/1000, score: -68.43563712067329, e: 0.10000000001321907\n",
      "episode: 71/1000, score: -174.378651405496, e: 0.10000000000798577\n",
      "episode: 72/1000, score: -119.28286981609455, e: 0.10000000000726932\n",
      "episode: 73/1000, score: -71.35111932089828, e: 0.10000000000610838\n",
      "episode: 74/1000, score: -40.86723584106181, e: 0.1000000000022494\n",
      "episode: 75/1000, score: -149.27350631730275, e: 0.1000000000019168\n",
      "episode: 76/1000, score: -106.92062519128135, e: 0.10000000000070587\n",
      "episode: 77/1000, score: -121.86642019207807, e: 0.10000000000056478\n",
      "episode: 78/1000, score: -74.27962442714276, e: 0.10000000000020798\n",
      "episode: 79/1000, score: -63.99980604523081, e: 0.1000000000000766\n",
      "episode: 80/1000, score: -128.2015689292168, e: 0.1000000000000282\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "batch_size = 32\n",
    "EPISODES = 1000\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    R = 0\n",
    "    for time in range(150000):\n",
    "        # env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward\n",
    "        R += reward\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.add_memory(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {}\".format(e, EPISODES, R, agent.epsilon))\n",
    "            break\n",
    "        agent.batch_run()\n",
    "        \n",
    "for e in range(200):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    R = 0\n",
    "    for time in range(150000):\n",
    "        # env.render()\n",
    "        action = agent.act(state, test)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward\n",
    "        R += reward\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.add_memory(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {}\".format(e, EPISODES, R, agent.epsilon))\n",
    "            break\n",
    "        agent.batch_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
