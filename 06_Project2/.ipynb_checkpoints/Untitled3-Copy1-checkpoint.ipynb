{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size, memory=20000, \n",
    "                 gamma=0.99, max_eps=1, min_eps=0.1, decay=0.001,\n",
    "                 lr=0.00025, batch_size=32):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=memory)\n",
    "        \n",
    "        self.max_eps = max_eps\n",
    "        self.epsilon = max_eps\n",
    "        self.min_eps = min_eps\n",
    "        \n",
    "        self.decay = decay\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = 0\n",
    "                \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(self.state_size,),\n",
    "                            activation = 'relu'))\n",
    "        self.model.add(Dense(24, activation='relu'))\n",
    "        self.model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "\n",
    "        self.model.compile(optimizer=Adam(lr=lr), loss='mse')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def act(self, state, test=False):\n",
    "        \n",
    "        if test:\n",
    "            probs = self.model.predict(state)\n",
    "            return np.argmax(probs[0])         \n",
    "        \n",
    "        # explore\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # exploit\n",
    "        else:\n",
    "            probs = self.model.predict(state)\n",
    "            return np.argmax(probs[0])\n",
    "        \n",
    "    def add_memory(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    \n",
    "    def batch_run(self):\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        states = []\n",
    "        targets = []\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            state = batch[i][0]\n",
    "            action = batch[i][1]\n",
    "            reward = batch[i][2]\n",
    "            next_state = batch[i][3]\n",
    "            done = batch[i][4]\n",
    "            \n",
    "            probs = self.model.predict(next_state)[0]\n",
    "            target_n = reward + self.gamma * np.amax(probs)\n",
    "            \n",
    "            if done:\n",
    "                target_n = reward\n",
    "                \n",
    "            target = self.model.predict(state)\n",
    "            target[0][action] = target_n\n",
    "            \n",
    "            states.append(state[0])\n",
    "            targets.append(target[0])\n",
    "            \n",
    "        self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n",
    "        \n",
    "        self.steps += 1\n",
    "        self.epsilon = self.min_eps + (self.max_eps - self.min_eps \\\n",
    "                                      ) * math.exp(-self.decay * self.steps)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = Agent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1000, score: -62.33958035658112, e: 0.9391544379153535\n",
      "episode: 1/1000, score: -386.66609258636475, e: 0.8684648037716335\n",
      "episode: 2/1000, score: -129.89432015880783, e: 0.8108026065395224\n",
      "episode: 3/1000, score: -396.9876452124879, e: 0.7254020752686196\n",
      "episode: 4/1000, score: -396.74961775916756, e: 0.6653215946706766\n",
      "episode: 5/1000, score: -71.0394358849822, e: 0.6120359120612096\n",
      "episode: 6/1000, score: -442.54568480602416, e: 0.5514184621594499\n",
      "episode: 7/1000, score: -202.1437109850732, e: 0.4466774342509796\n",
      "episode: 8/1000, score: -225.21012062170087, e: 0.40809086734940914\n",
      "episode: 9/1000, score: -142.74011919284084, e: 0.33707931872899877\n",
      "episode: 10/1000, score: -306.882126933604, e: 0.30693293666805144\n",
      "episode: 11/1000, score: -283.1836767591879, e: 0.2829832764825576\n",
      "episode: 12/1000, score: -453.9006648828327, e: 0.2367828679858308\n",
      "episode: 13/1000, score: -171.8930294567121, e: 0.20224733830368735\n",
      "episode: 14/1000, score: -249.03161087056074, e: 0.17529357180183658\n",
      "episode: 15/1000, score: -217.5758360872874, e: 0.16751803607679427\n",
      "episode: 16/1000, score: -292.4527858730049, e: 0.15994303912110472\n",
      "episode: 17/1000, score: -137.21252441780817, e: 0.1482982227214571\n",
      "episode: 18/1000, score: -296.1546167744641, e: 0.14152911565637466\n",
      "episode: 19/1000, score: -143.27304079103857, e: 0.13507170792155673\n",
      "episode: 20/1000, score: -104.68323679631426, e: 0.12970735812154519\n",
      "episode: 21/1000, score: -153.99104027628132, e: 0.12608589543124965\n",
      "episode: 22/1000, score: -127.4719674855377, e: 0.12033604166600968\n",
      "episode: 23/1000, score: -264.1181205584809, e: 0.11755598229475349\n",
      "episode: 24/1000, score: -142.31279748342348, e: 0.11494526852802119\n",
      "episode: 25/1000, score: -155.96714426215325, e: 0.11135200934799634\n",
      "episode: 26/1000, score: -532.1854544363398, e: 0.10757152699176219\n",
      "episode: 27/1000, score: -136.30483391327454, e: 0.10278819592083142\n",
      "episode: 28/1000, score: -202.65932424853133, e: 0.10155332029137598\n",
      "episode: 29/1000, score: -100.35063422609687, e: 0.10057200632116498\n",
      "episode: 30/1000, score: -148.55470274369108, e: 0.10021063990039225\n",
      "episode: 31/1000, score: -111.92327730806608, e: 0.10013112486567431\n",
      "episode: 32/1000, score: -69.9758900163688, e: 0.10004828640457737\n",
      "episode: 33/1000, score: -296.6918264747319, e: 0.10003368828134472\n",
      "episode: 34/1000, score: -374.10334590150387, e: 0.10002700845708437\n",
      "episode: 35/1000, score: -165.84185879716082, e: 0.10000994579692479\n",
      "episode: 36/1000, score: -511.2202950942812, e: 0.10000784714712606\n",
      "episode: 37/1000, score: -485.1791763371315, e: 0.10000317449969105\n",
      "episode: 38/1000, score: -392.1300334205414, e: 0.10000268626040476\n",
      "episode: 39/1000, score: -325.55991913314944, e: 0.10000241850611646\n",
      "episode: 40/1000, score: -304.876591874818, e: 0.10000159863484362\n",
      "episode: 41/1000, score: -171.93485396127164, e: 0.10000058869329195\n",
      "episode: 42/1000, score: -159.37742881316223, e: 0.10000039659092003\n",
      "episode: 43/1000, score: -219.42904472507868, e: 0.10000033192376574\n",
      "episode: 44/1000, score: -106.10364346923106, e: 0.10000021355733002\n",
      "episode: 45/1000, score: -265.3272924165568, e: 0.10000017155524128\n",
      "episode: 46/1000, score: -64.3398810700169, e: 0.10000009452887292\n",
      "episode: 47/1000, score: -231.8584098662445, e: 0.10000008476691567\n",
      "episode: 48/1000, score: -328.1570877152168, e: 0.10000007245062688\n",
      "episode: 49/1000, score: -350.83917343914084, e: 0.10000006106295135\n",
      "episode: 50/1000, score: -665.2211483741163, e: 0.10000004861369695\n",
      "episode: 51/1000, score: -668.1567201286186, e: 0.10000004080910921\n",
      "episode: 52/1000, score: -370.0520230266731, e: 0.10000003321180288\n",
      "episode: 53/1000, score: -327.7722562955746, e: 0.10000002740992904\n",
      "episode: 54/1000, score: -16.39314237918208, e: 0.10000001009363797\n",
      "episode: 55/1000, score: -312.21679040715856, e: 0.10000000764387995\n",
      "episode: 56/1000, score: -103.14468762827323, e: 0.10000000281483973\n",
      "episode: 57/1000, score: -125.01852920548144, e: 0.1000000010365577\n",
      "episode: 58/1000, score: -80.33765211044371, e: 0.1000000003817098\n",
      "episode: 59/1000, score: -144.60411584515035, e: 0.10000000014056368\n",
      "episode: 60/1000, score: -223.46663166830842, e: 0.10000000012268997\n",
      "episode: 61/1000, score: -125.80766142646775, e: 0.10000000004518028\n",
      "episode: 62/1000, score: -258.8826222695046, e: 0.10000000003431768\n",
      "episode: 63/1000, score: -254.52329643357152, e: 0.10000000002877943\n",
      "episode: 64/1000, score: -220.0248060819555, e: 0.10000000002166419\n",
      "episode: 65/1000, score: -95.40030699172897, e: 0.10000000000797779\n",
      "episode: 66/1000, score: -195.8234977854362, e: 0.10000000000701227\n",
      "episode: 67/1000, score: -230.53139968650453, e: 0.10000000000572969\n",
      "episode: 68/1000, score: -298.2371520227259, e: 0.10000000000451165\n",
      "episode: 69/1000, score: -137.8588193433509, e: 0.1000000000016614\n",
      "episode: 70/1000, score: -78.69348797697292, e: 0.10000000000061181\n",
      "episode: 71/1000, score: 59.160927357376586, e: 0.10000000000024334\n",
      "episode: 72/1000, score: -225.109752224953, e: 0.10000000000012628\n",
      "episode: 73/1000, score: -130.15711527024797, e: 0.10000000000008802\n",
      "episode: 74/1000, score: -240.74880482545174, e: 0.10000000000003721\n",
      "episode: 75/1000, score: -268.1089234436728, e: 0.10000000000001696\n",
      "episode: 76/1000, score: -139.55309058586005, e: 0.10000000000001265\n",
      "episode: 77/1000, score: -144.48858234521197, e: 0.10000000000000804\n",
      "episode: 78/1000, score: -159.1487725053516, e: 0.1000000000000058\n",
      "episode: 79/1000, score: -213.5440918048435, e: 0.10000000000000318\n",
      "episode: 80/1000, score: -133.42298876566028, e: 0.10000000000000259\n",
      "episode: 81/1000, score: -166.94232681148685, e: 0.10000000000000166\n",
      "episode: 82/1000, score: -147.62193996466993, e: 0.10000000000000062\n",
      "episode: 83/1000, score: -230.928732707394, e: 0.10000000000000023\n",
      "episode: 84/1000, score: -299.50743543108297, e: 0.10000000000000014\n",
      "episode: 85/1000, score: -183.93924729350263, e: 0.10000000000000013\n",
      "episode: 86/1000, score: -256.06493709492736, e: 0.10000000000000012\n",
      "episode: 87/1000, score: -230.92753815540752, e: 0.10000000000000007\n",
      "episode: 88/1000, score: -207.83683951431624, e: 0.10000000000000006\n",
      "episode: 89/1000, score: -111.42243848900165, e: 0.10000000000000005\n",
      "episode: 90/1000, score: -228.38991997449244, e: 0.10000000000000003\n",
      "episode: 91/1000, score: -293.90064113780693, e: 0.10000000000000003\n",
      "episode: 92/1000, score: -214.63061780436638, e: 0.10000000000000002\n",
      "episode: 93/1000, score: -97.57771716993982, e: 0.1\n",
      "episode: 94/1000, score: -262.9460824414482, e: 0.1\n",
      "episode: 95/1000, score: -169.9655900493529, e: 0.1\n",
      "episode: 96/1000, score: -198.1775780938524, e: 0.1\n",
      "episode: 97/1000, score: -243.68337536663327, e: 0.1\n",
      "episode: 98/1000, score: -132.33338785754643, e: 0.1\n",
      "episode: 99/1000, score: -118.52115930951834, e: 0.1\n",
      "episode: 100/1000, score: -72.55883032205803, e: 0.1\n",
      "episode: 101/1000, score: -109.63591383852679, e: 0.1\n",
      "episode: 102/1000, score: -134.088695775789, e: 0.1\n",
      "episode: 103/1000, score: -72.51726026698198, e: 0.1\n",
      "episode: 104/1000, score: -70.38587254835318, e: 0.1\n",
      "episode: 105/1000, score: -136.76027817087623, e: 0.1\n",
      "episode: 106/1000, score: -126.05243690655743, e: 0.1\n",
      "episode: 107/1000, score: -136.68325013692905, e: 0.1\n",
      "episode: 108/1000, score: -104.96634455859144, e: 0.1\n",
      "episode: 109/1000, score: -92.95186647452678, e: 0.1\n",
      "episode: 110/1000, score: -111.03332807450852, e: 0.1\n",
      "episode: 111/1000, score: -176.9714703198582, e: 0.1\n",
      "episode: 112/1000, score: -118.91666441196666, e: 0.1\n",
      "episode: 113/1000, score: -132.00249988403212, e: 0.1\n",
      "episode: 114/1000, score: -120.06525094672995, e: 0.1\n",
      "episode: 115/1000, score: -161.67151545364663, e: 0.1\n",
      "episode: 116/1000, score: -105.74920921526585, e: 0.1\n",
      "episode: 117/1000, score: -167.3371335869201, e: 0.1\n",
      "episode: 118/1000, score: -350.40948663797116, e: 0.1\n",
      "episode: 119/1000, score: -128.28990464458013, e: 0.1\n",
      "episode: 120/1000, score: -113.60896712387978, e: 0.1\n",
      "episode: 121/1000, score: -94.53672563291423, e: 0.1\n",
      "episode: 122/1000, score: -102.31418243151614, e: 0.1\n",
      "episode: 123/1000, score: -119.00255335943376, e: 0.1\n",
      "episode: 124/1000, score: -102.33595100374345, e: 0.1\n",
      "episode: 125/1000, score: -94.06213890406059, e: 0.1\n",
      "episode: 126/1000, score: -84.06456453528357, e: 0.1\n",
      "episode: 127/1000, score: -84.69750750921344, e: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 128/1000, score: -103.72951961541685, e: 0.1\n",
      "episode: 129/1000, score: -101.25277702823482, e: 0.1\n",
      "episode: 130/1000, score: -147.19285033464803, e: 0.1\n",
      "episode: 131/1000, score: -116.69676099797023, e: 0.1\n",
      "episode: 132/1000, score: -68.27911426350248, e: 0.1\n",
      "episode: 133/1000, score: -110.56532809137148, e: 0.1\n",
      "episode: 134/1000, score: -96.2122878050814, e: 0.1\n",
      "episode: 135/1000, score: -256.07751474383934, e: 0.1\n",
      "episode: 136/1000, score: -506.05621324466676, e: 0.1\n",
      "episode: 137/1000, score: -277.2269612098265, e: 0.1\n",
      "episode: 138/1000, score: -460.5817815861872, e: 0.1\n",
      "episode: 139/1000, score: -461.14919927956686, e: 0.1\n",
      "episode: 140/1000, score: -99.47695145310544, e: 0.1\n",
      "episode: 141/1000, score: -366.3259601891566, e: 0.1\n",
      "episode: 142/1000, score: -378.6306550823891, e: 0.1\n",
      "episode: 143/1000, score: -249.56264028461138, e: 0.1\n",
      "episode: 144/1000, score: -123.23732487137934, e: 0.1\n",
      "episode: 145/1000, score: -427.6810255912093, e: 0.1\n",
      "episode: 146/1000, score: -167.86037841639222, e: 0.1\n",
      "episode: 147/1000, score: -255.68013960806576, e: 0.1\n",
      "episode: 148/1000, score: -186.85306656334723, e: 0.1\n",
      "episode: 149/1000, score: -81.61040255948629, e: 0.1\n",
      "episode: 150/1000, score: -385.1402121924818, e: 0.1\n",
      "episode: 151/1000, score: -78.25636021227744, e: 0.1\n",
      "episode: 152/1000, score: -126.47158691426816, e: 0.1\n",
      "episode: 153/1000, score: -54.60782830856866, e: 0.1\n",
      "episode: 154/1000, score: -343.62378078910604, e: 0.1\n",
      "episode: 155/1000, score: -217.69764019545448, e: 0.1\n",
      "episode: 156/1000, score: -248.24335921917734, e: 0.1\n",
      "episode: 157/1000, score: -229.37242585880284, e: 0.1\n",
      "episode: 158/1000, score: -431.7941426489092, e: 0.1\n",
      "episode: 159/1000, score: -174.8838940013279, e: 0.1\n",
      "episode: 160/1000, score: -185.31918402029498, e: 0.1\n",
      "episode: 161/1000, score: -352.9750561856308, e: 0.1\n",
      "episode: 162/1000, score: -122.62275391957027, e: 0.1\n",
      "episode: 163/1000, score: -315.6696493088572, e: 0.1\n",
      "episode: 164/1000, score: -372.81397070869417, e: 0.1\n",
      "episode: 165/1000, score: -405.5871288295335, e: 0.1\n",
      "episode: 166/1000, score: -391.7582282138655, e: 0.1\n",
      "episode: 167/1000, score: -357.08173983357455, e: 0.1\n",
      "episode: 168/1000, score: -185.09636300136222, e: 0.1\n",
      "episode: 169/1000, score: -163.93143642474578, e: 0.1\n",
      "episode: 170/1000, score: -395.21847033828135, e: 0.1\n",
      "episode: 171/1000, score: -173.6880176235479, e: 0.1\n",
      "episode: 172/1000, score: -181.92136333291222, e: 0.1\n",
      "episode: 173/1000, score: -193.11631027395322, e: 0.1\n",
      "episode: 174/1000, score: -212.17885451883555, e: 0.1\n",
      "episode: 175/1000, score: -402.584187645237, e: 0.1\n",
      "episode: 176/1000, score: -158.95179196271945, e: 0.1\n",
      "episode: 177/1000, score: -188.57375279257585, e: 0.1\n",
      "episode: 178/1000, score: -180.7343149033623, e: 0.1\n",
      "episode: 179/1000, score: -93.05177353566603, e: 0.1\n",
      "episode: 180/1000, score: -208.98018276831985, e: 0.1\n",
      "episode: 181/1000, score: -160.56502319002385, e: 0.1\n",
      "episode: 182/1000, score: -335.8745668622108, e: 0.1\n",
      "episode: 183/1000, score: -275.13245445269933, e: 0.1\n",
      "episode: 184/1000, score: -188.1458287399193, e: 0.1\n",
      "episode: 185/1000, score: -131.4107052636672, e: 0.1\n",
      "episode: 186/1000, score: -126.013186763904, e: 0.1\n",
      "episode: 187/1000, score: -59.41489693753144, e: 0.1\n",
      "episode: 188/1000, score: -163.06908185366396, e: 0.1\n",
      "episode: 189/1000, score: -120.91650626011365, e: 0.1\n",
      "episode: 190/1000, score: -384.85090137729526, e: 0.1\n",
      "episode: 191/1000, score: -161.20480716166045, e: 0.1\n",
      "episode: 192/1000, score: -166.37064518278655, e: 0.1\n",
      "episode: 193/1000, score: -167.48396411823353, e: 0.1\n",
      "episode: 194/1000, score: -138.60311994523678, e: 0.1\n",
      "episode: 195/1000, score: -80.47820339390906, e: 0.1\n",
      "episode: 196/1000, score: -84.69848339354893, e: 0.1\n",
      "episode: 197/1000, score: -111.53582586552444, e: 0.1\n",
      "episode: 198/1000, score: -130.18787400830507, e: 0.1\n",
      "episode: 199/1000, score: -107.11698100343435, e: 0.1\n",
      "episode: 200/1000, score: -75.9198775102527, e: 0.1\n",
      "episode: 201/1000, score: -85.23459225503922, e: 0.1\n",
      "episode: 202/1000, score: -157.15126232870176, e: 0.1\n",
      "episode: 203/1000, score: -156.6402874772685, e: 0.1\n",
      "episode: 204/1000, score: -58.205808326105256, e: 0.1\n",
      "episode: 205/1000, score: -115.93467958427986, e: 0.1\n",
      "episode: 206/1000, score: -200.45452790713557, e: 0.1\n",
      "episode: 207/1000, score: -92.56678934159183, e: 0.1\n",
      "episode: 208/1000, score: -56.064517967505324, e: 0.1\n",
      "episode: 209/1000, score: -86.75893185286277, e: 0.1\n",
      "episode: 210/1000, score: -96.0005257575118, e: 0.1\n",
      "episode: 211/1000, score: -89.12618941898549, e: 0.1\n",
      "episode: 212/1000, score: -111.10476018988174, e: 0.1\n",
      "episode: 213/1000, score: -119.81183557908186, e: 0.1\n",
      "episode: 214/1000, score: -144.60998075285806, e: 0.1\n",
      "episode: 215/1000, score: -139.01251294505056, e: 0.1\n",
      "episode: 216/1000, score: -154.43438401292644, e: 0.1\n",
      "episode: 217/1000, score: -144.42185213240722, e: 0.1\n",
      "episode: 218/1000, score: -112.23503340456082, e: 0.1\n",
      "episode: 219/1000, score: -148.48170011323654, e: 0.1\n",
      "episode: 220/1000, score: -112.56288885202238, e: 0.1\n",
      "episode: 221/1000, score: -253.5397131887023, e: 0.1\n",
      "episode: 222/1000, score: -118.08054561978105, e: 0.1\n",
      "episode: 223/1000, score: -266.6558707531718, e: 0.1\n",
      "episode: 224/1000, score: -63.91333129781596, e: 0.1\n",
      "episode: 225/1000, score: -85.91617338831874, e: 0.1\n",
      "episode: 226/1000, score: -154.66416643013898, e: 0.1\n",
      "episode: 227/1000, score: -67.76691089361645, e: 0.1\n",
      "episode: 228/1000, score: -66.57046873769653, e: 0.1\n",
      "episode: 229/1000, score: -130.4225651789477, e: 0.1\n",
      "episode: 230/1000, score: -58.62089647413572, e: 0.1\n",
      "episode: 231/1000, score: -183.9793725265606, e: 0.1\n",
      "episode: 232/1000, score: -118.45795955971072, e: 0.1\n",
      "episode: 233/1000, score: -37.41548337880746, e: 0.1\n",
      "episode: 234/1000, score: -109.28916305261107, e: 0.1\n",
      "episode: 235/1000, score: -105.16453036264883, e: 0.1\n",
      "episode: 236/1000, score: -94.7177253402036, e: 0.1\n",
      "episode: 237/1000, score: -80.02840754257768, e: 0.1\n",
      "episode: 238/1000, score: -105.37948866804366, e: 0.1\n",
      "episode: 239/1000, score: -79.26361591756692, e: 0.1\n",
      "episode: 240/1000, score: -177.1067047404255, e: 0.1\n",
      "episode: 241/1000, score: -105.21342496225608, e: 0.1\n",
      "episode: 242/1000, score: -108.38304089244748, e: 0.1\n",
      "episode: 243/1000, score: -143.50011717158884, e: 0.1\n",
      "episode: 244/1000, score: -194.64196846342597, e: 0.1\n",
      "episode: 245/1000, score: -128.29259343724138, e: 0.1\n",
      "episode: 246/1000, score: -118.1287958461711, e: 0.1\n",
      "episode: 247/1000, score: -84.45012603449479, e: 0.1\n",
      "episode: 248/1000, score: -92.56238336086646, e: 0.1\n",
      "episode: 249/1000, score: -103.16867610578355, e: 0.1\n",
      "episode: 250/1000, score: -94.67137670050869, e: 0.1\n",
      "episode: 251/1000, score: -93.94648692629593, e: 0.1\n",
      "episode: 252/1000, score: 41.751398212687256, e: 0.1\n",
      "episode: 253/1000, score: -197.63273972253677, e: 0.1\n",
      "episode: 254/1000, score: -133.21268093596328, e: 0.1\n",
      "episode: 255/1000, score: -39.415537965187994, e: 0.1\n",
      "episode: 256/1000, score: -57.39709732012521, e: 0.1\n",
      "episode: 257/1000, score: -41.14454588065612, e: 0.1\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "batch_size = 32\n",
    "EPISODES = 1000\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    R = 0\n",
    "    for time in range(150000):\n",
    "        # env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward\n",
    "        R += reward\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.add_memory(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {}\".format(e, EPISODES, R, agent.epsilon))\n",
    "            break\n",
    "        agent.batch_run()\n",
    "        \n",
    "for e in range(200):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    R = 0\n",
    "    for time in range(150000):\n",
    "        # env.render()\n",
    "        action = agent.act(state, test)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward\n",
    "        R += reward\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.add_memory(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {}\".format(e, EPISODES, R, agent.epsilon))\n",
    "            break\n",
    "        agent.batch_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running():\n",
    "    done = False\n",
    "    EPISODES = 1000\n",
    "    for e in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        R = 0\n",
    "        for time in range(150000):\n",
    "            # env.render()\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = reward\n",
    "            R += reward\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.add_memory(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}, e: {}\".format(e, EPISODES, R, agent.epsilon))\n",
    "                break\n",
    "            agent.batch_run()\n",
    "    RR = np.zeros(200)\n",
    "    for e in range(200):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        R = 0\n",
    "        for time in range(150000):\n",
    "            # env.render()\n",
    "            action = agent.act(state, test)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = reward\n",
    "            R += reward\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.add_memory(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}, e: {}\".format(e, EPISODES, R, agent.epsilon))\n",
    "                RR[e] = R\n",
    "                break\n",
    "            agent.batch_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
