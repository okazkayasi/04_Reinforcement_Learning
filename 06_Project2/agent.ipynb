{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size, memory=20000, \n",
    "                 gamma=0.99, max_eps=1, min_eps=0.1, decay=0.001,\n",
    "                 lr=0.00025, batch_size=32):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=memory)\n",
    "        \n",
    "        self.max_eps = max_eps\n",
    "        self.epsilon = max_eps\n",
    "        self.min_eps = min_eps\n",
    "        \n",
    "        self.decay = decay\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = 0\n",
    "                \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(400, input_shape=(self.state_size,),\n",
    "                            activation = 'relu'))\n",
    "        self.model.add(Dense(200, activation='relu'))\n",
    "        self.model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "\n",
    "        self.model.compile(optimizer=Adam(lr=lr), loss='mse')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def act(self, state, test=False):\n",
    "        \n",
    "        if test:\n",
    "            probs = self.model.predict(state)\n",
    "            return np.argmax(probs[0])         \n",
    "        \n",
    "        # explore\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # exploit\n",
    "        else:\n",
    "            probs = self.model.predict(state)\n",
    "            return np.argmax(probs[0])\n",
    "        \n",
    "    def add_memory(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    \n",
    "    def batch_run(self):\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        states = []\n",
    "        targets = []\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            state = batch[i][0]\n",
    "            action = batch[i][1]\n",
    "            reward = batch[i][2]\n",
    "            next_state = batch[i][3]\n",
    "            done = batch[i][4]\n",
    "            \n",
    "            probs = self.model.predict(next_state)[0]\n",
    "            target_n = reward + self.gamma * np.amax(probs)\n",
    "            \n",
    "            if done:\n",
    "                target_n = reward\n",
    "                \n",
    "            target = self.model.predict(state)\n",
    "            target[0][action] = target_n\n",
    "            \n",
    "            states.append(state[0])\n",
    "            targets.append(target[0])\n",
    "            \n",
    "        self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n",
    "        \n",
    "        self.steps += 1\n",
    "        self.epsilon = self.min_eps + (self.max_eps - self.min_eps \\\n",
    "                                      ) * math.exp(-self.decay * self.steps)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = Agent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -311.1311495062664 -100 0.9184356410214083\n",
      "1 -106.72216322838148 -100 0.8547561850033666\n",
      "2 -112.0670113330076 -100 0.7842988677347938\n",
      "3 -192.13321330422554 -100 0.7310560988877152\n",
      "4 -239.37052364494622 -100 0.6784746719099781\n",
      "5 -145.3440035544929 -100 0.5890157821670499\n",
      "6 -195.40436680559012 -100 0.5268338226352179\n",
      "7 -420.7253261585752 -100 0.4800849435220095\n",
      "8 -331.69698710302373 -100 0.43643156765091984\n",
      "9 -94.62703420488683 -100 0.37517156154884057\n",
      "10 -93.16218453485034 -100 0.3130218544130568\n",
      "11 -73.4316016981215 -1.7982364810483518 0.17844476631578318\n",
      "12 -76.33710921263473 -1.749647618298613 0.12888708944578908\n",
      "13 -57.438678065121465 -2.8582463038041355 0.11063759860396351\n",
      "14 -99.593279264369 1.4823482817948854 0.10391726914099135\n",
      "15 -267.01816656139897 -100 0.1014939068917623\n",
      "16 -62.64482104476486 -1.72630796077421 0.10055012748501663\n",
      "17 -46.19211064334441 2.8913139189662345 0.10020258307357677\n",
      "18 -24.364471473314165 -1.0582940615690888 0.10007460071132161\n",
      "19 -18.465694608428198 0.14210890544995294 0.10002747152578659\n",
      "20 -43.75073876483335 0.22380610858595845 0.10001011632081884\n",
      "21 -21.552463868539547 0.4571249296300664 0.10000372530989741\n",
      "22 -47.36042358130646 8.106891003338562 0.10000137183607363\n",
      "23 154.34351282388403 100 0.10000052003978657\n",
      "24 -98.27545623586133 -100 0.10000041153962985\n",
      "25 -67.25728235685081 -100 0.10000032405242923\n",
      "26 146.08900969403635 100 0.10000014604361666\n",
      "27 -116.20386834819038 -100 0.10000011568897282\n",
      "28 -158.4533518786144 -100 0.10000008734845259\n",
      "29 -18.333588113953407 -0.35876809707919505 0.1000000321658497\n",
      "30 -86.7177228847862 -100 0.10000001990369357\n",
      "31 -185.88259975410435 -100 0.10000001180951215\n",
      "32 -153.57455037694135 -100 0.10000000605512682\n",
      "33 -68.7750608633136 -100 0.10000000348652426\n",
      "34 -175.33784844934536 -100 0.10000000146507337\n",
      "35 -146.2704264335093 -100 0.10000000106492601\n",
      "36 -145.43508456650403 -100 0.10000000080727195\n",
      "37 -65.03881356544886 -100 0.10000000052777001\n",
      "38 69.87951232108053 -0.4545960868879948 0.10000000019435\n",
      "39 -45.42737241224104 -0.3758337717445556 0.10000000007156891\n",
      "40 21.75190391049646 -0.2882315202240242 0.10000000002635508\n",
      "41 -61.347194408676934 -0.44945602837617854 0.1000000000097052\n",
      "42 -116.51773492754151 2.335048200192915 0.10000000000357391\n",
      "43 18.809305780583315 -0.06000805192323952 0.10000000000131609\n",
      "44 44.11681864853834 1.3987141426853331 0.10000000000048465\n",
      "45 -163.44284522267114 -100 0.1000000000002371\n",
      "46 21.343931430279994 -0.6208148638785331 0.10000000000008731\n",
      "47 -14.067235115037642 0.19622931812777125 0.10000000000003216\n",
      "48 -41.88109596768371 -1.7705678359361372 0.10000000000001184\n",
      "49 -3.3268569145878244 -0.41704085370661587 0.10000000000000436\n",
      "50 -17.93959840797423 1.098722090353818 0.10000000000000162\n",
      "51 208.7218517891369 100 0.10000000000000092\n",
      "52 161.310983385687 100 0.10000000000000044\n",
      "53 -441.603516350579 -100 0.10000000000000017\n",
      "54 22.576034852278344 -1.5848187902256747 0.10000000000000006\n",
      "55 283.17865507764736 100 0.10000000000000005\n",
      "56 217.41922082197135 100 0.10000000000000002\n",
      "57 194.75918077394272 100 0.10000000000000002\n",
      "58 260.6411758359211 100 0.1\n",
      "59 182.96596805935883 100 0.1\n",
      "60 24.288296007960476 1.074324373233176 0.1\n",
      "61 107.57315256961293 100 0.1\n",
      "62 210.81202098745928 100 0.1\n",
      "63 11.043712115596808 1.6912800070672247 0.1\n",
      "64 -30.784749747107085 -100 0.1\n",
      "65 202.19601293467096 100 0.1\n",
      "66 263.76169911032866 100 0.1\n",
      "67 170.92986608105045 100 0.1\n",
      "68 48.319741388221644 -2.3321804576426075e-07 0.1\n",
      "69 94.9516151250763 100 0.1\n",
      "70 258.428083509731 100 0.1\n",
      "71 171.93955013533844 100 0.1\n",
      "72 232.4145704646304 100 0.1\n",
      "73 196.67067861700468 100 0.1\n",
      "74 238.36673906554566 100 0.1\n",
      "75 223.3238365293048 100 0.1\n",
      "76 244.97152828883986 100 0.1\n",
      "77 126.38878037503444 100 0.1\n",
      "78 287.01753932961947 100 0.1\n",
      "79 173.94936361266036 100 0.1\n",
      "80 235.74230423924348 100 0.1\n",
      "81 219.16475489884164 100 0.1\n",
      "82 235.27645852495647 100 0.1\n",
      "83 225.82308516770007 100 0.1\n",
      "84 231.04086800808096 100 0.1\n",
      "85 239.6035654221635 100 0.1\n",
      "86 274.17766379237975 100 0.1\n",
      "87 -8.355634977326702 -100 0.1\n",
      "88 187.7320110608755 100 0.1\n",
      "89 245.84970718661341 100 0.1\n",
      "90 220.39600294666303 100 0.1\n",
      "91 207.80965648592206 100 0.1\n",
      "92 31.447461304729245 -100 0.1\n",
      "93 223.4355817853591 100 0.1\n",
      "94 -1.7257547343761246 -100 0.1\n",
      "95 242.8039131851187 100 0.1\n",
      "96 271.3624955637348 100 0.1\n",
      "97 -31.749926542901306 -100 0.1\n",
      "98 212.83073741656608 100 0.1\n",
      "99 -49.4503777423114 -100 0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-593ff415fb06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mTEST\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-03202ccd0698>\u001b[0m in \u001b[0;36mbatch_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mtarget_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/p2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda2/envs/p2/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/p2/lib/python3.6/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/p2/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    811\u001b[0m                         s.storage[0] = s.type.filter(\n\u001b[1;32m    812\u001b[0m                             \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m                             allow_downcast=s.allow_downcast)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/p2/lib/python3.6/site-packages/theano/tensor/type.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, data, strict, allow_downcast)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Explicit error message when one accidentally uses a Variable as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# input (typical mistake, especially with shared variables).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             raise TypeError(\n\u001b[1;32m     87\u001b[0m                 \u001b[0;34m'Expected an array-like object, but found a Variable: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "done = False\n",
    "batch_size = 32\n",
    "TRAIN = 1000\n",
    "r = np.zeros(TRAIN)\n",
    "for e in range(TRAIN):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    R = 0\n",
    "    for time in range(150000):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.add_memory(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(e, R, reward, agent.epsilon)\n",
    "            r[e] = R\n",
    "\n",
    "            break\n",
    "        agent.batch_run()\n",
    "\n",
    "TEST = 100\n",
    "r_test = np.zeros(TEST)\n",
    "for e in range(TEST):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    R = 0\n",
    "    for time in range(150000):\n",
    "        tet = True\n",
    "        # env.render()\n",
    "        action = agent.act(state, test=True)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.add_memory(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(e, R, reward, agent.epsilon)\n",
    "            r_test[e] = R\n",
    "\n",
    "            break\n",
    "        agent.batch_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269.61857044378775"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(r_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
